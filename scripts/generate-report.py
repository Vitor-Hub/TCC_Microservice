#!/usr/bin/env python3
"""
üìä Gerador de Relat√≥rio Visual para TCC - VERS√ÉO APRIMORADA
Compara m√©tricas entre Microsservi√ßos e Monol√≠tico com an√°lises estat√≠sticas detalhadas
"""

import json
import sys
import os
from pathlib import Path
from datetime import datetime
import argparse
from typing import Dict, Any, Optional, List

def load_k6_summary(filepath: str) -> Optional[Dict[str, Any]]:
    """Carrega o arquivo summary JSON do K6"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"‚ùå Erro: Arquivo n√£o encontrado: {filepath}")
        return None
    except json.JSONDecodeError as e:
        print(f"‚ùå Erro ao decodificar JSON: {e}")
        return None
    except Exception as e:
        print(f"‚ùå Erro ao carregar {filepath}: {e}")
        return None

def extract_metrics(summary: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """Extrai m√©tricas importantes do summary do K6"""
    if not summary or 'metrics' not in summary:
        return None
    
    metrics = summary['metrics']
    
    result = {
        'http_reqs': {
            'count': metrics.get('http_reqs', {}).get('values', {}).get('count', 0),
            'rate': metrics.get('http_reqs', {}).get('values', {}).get('rate', 0),
        },
        'http_req_duration': {
            'avg': metrics.get('http_req_duration', {}).get('values', {}).get('avg', 0),
            'med': metrics.get('http_req_duration', {}).get('values', {}).get('med', 0),
            'p90': metrics.get('http_req_duration', {}).get('values', {}).get('p(90)', 0),
            'p95': metrics.get('http_req_duration', {}).get('values', {}).get('p(95)', 0),
            'p99': metrics.get('http_req_duration', {}).get('values', {}).get('p(99)', 0),
            'min': metrics.get('http_req_duration', {}).get('values', {}).get('min', 0),
            'max': metrics.get('http_req_duration', {}).get('values', {}).get('max', 0),
        },
        'http_req_failed': {
            'rate': metrics.get('http_req_failed', {}).get('values', {}).get('rate', 0) * 100,
        },
        'http_req_waiting': {
            'avg': metrics.get('http_req_waiting', {}).get('values', {}).get('avg', 0),
        },
        'http_req_blocked': {
            'avg': metrics.get('http_req_blocked', {}).get('values', {}).get('avg', 0),
        },
        'http_req_connecting': {
            'avg': metrics.get('http_req_connecting', {}).get('values', {}).get('avg', 0),
        },
        'iterations': {
            'count': metrics.get('iterations', {}).get('values', {}).get('count', 0),
            'rate': metrics.get('iterations', {}).get('values', {}).get('rate', 0),
        },
        'vus': {
            'max': metrics.get('vus', {}).get('values', {}).get('max', 0),
            'min': metrics.get('vus', {}).get('values', {}).get('min', 0),
        },
        'data_received': {
            'count': metrics.get('data_received', {}).get('values', {}).get('count', 0),
            'rate': metrics.get('data_received', {}).get('values', {}).get('rate', 0),
        },
        'data_sent': {
            'count': metrics.get('data_sent', {}).get('values', {}).get('count', 0),
            'rate': metrics.get('data_sent', {}).get('values', {}).get('rate', 0),
        },
    }
    
    # M√©tricas customizadas (se existirem)
    custom_metrics = [
        'user_creation_duration', 'post_creation_duration', 
        'comment_creation_duration', 'like_creation_duration',
        'friendship_creation_duration', 'feed_load_duration',
        'get_operation_duration'
    ]
    
    for metric_name in custom_metrics:
        if metric_name in metrics:
            result[metric_name] = {
                'avg': metrics[metric_name].get('values', {}).get('avg', 0),
                'p95': metrics[metric_name].get('values', {}).get('p(95)', 0),
                'p99': metrics[metric_name].get('values', {}).get('p(99)', 0),
            }
    
    return result

def calculate_percentage_diff(val1: float, val2: float) -> float:
    """Calcula diferen√ßa percentual entre dois valores"""
    if val2 == 0:
        return 0.0
    return ((val1 - val2) / val2) * 100

def format_bytes(bytes_value: float) -> str:
    """Formata bytes em unidades leg√≠veis"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if bytes_value < 1024.0:
            return f"{bytes_value:.2f} {unit}"
        bytes_value /= 1024.0
    return f"{bytes_value:.2f} TB"

def generate_ascii_chart(data: Dict[str, float], title: str, max_width: int = 50) -> str:
    """Gera um gr√°fico ASCII simples"""
    if not data:
        return "Sem dados para exibir"
    
    max_value = max(data.values()) if data.values() else 1
    chart = [f"\n{title}\n{'‚ïê' * (max_width + 20)}\n"]
    
    for label, value in data.items():
        bar_length = int((value / max_value) * max_width) if max_value > 0 else 0
        bar = '‚ñà' * bar_length
        chart.append(f"{label:20} {bar} {value:.2f}\n")
    
    return ''.join(chart)

def generate_comparison_table(label: str, micro_val: float, mono_val: float, 
                             unit: str = "", lower_is_better: bool = True) -> str:
    """Gera uma linha de compara√ß√£o formatada"""
    diff = calculate_percentage_diff(micro_val, mono_val)
    
    if lower_is_better:
        winner = "‚úÖ Micro" if micro_val < mono_val else "‚úÖ Mono"
    else:
        winner = "‚úÖ Micro" if micro_val > mono_val else "‚úÖ Mono"
    
    return f"| {label:25} | {micro_val:.2f}{unit:5} | {mono_val:.2f}{unit:5} | {diff:+.2f}% | {winner} |\n"

def generate_markdown_report_single(metrics: Dict[str, Any], arch_name: str, output_dir: Path) -> Path:
    """Gera relat√≥rio Markdown apenas para uma arquitetura"""
    
    report = f"""# üìä Relat√≥rio de Desempenho - {arch_name}

**Data de Gera√ß√£o:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}

---

## üéØ Resumo Executivo

Este relat√≥rio apresenta m√©tricas de desempenho da arquitetura **{arch_name}** em um sistema de rede social simulada.

---

## üìà M√©tricas Gerais

### Total de Requisi√ß√µes

- **Total:** {metrics['http_reqs']['count']:,} requisi√ß√µes
- **Taxa:** {metrics['http_reqs']['rate']:.2f} req/s
- **Itera√ß√µes de VUs:** {metrics['iterations']['count']:,}
- **Taxa de Itera√ß√µes:** {metrics['iterations']['rate']:.2f} iter/s
- **VUs M√°ximos:** {metrics['vus']['max']}

---

## ‚è±Ô∏è Lat√™ncia de Requisi√ß√µes

| M√©trica | Valor (ms) |
|---------|------------|
| **M√©dia** | {metrics['http_req_duration']['avg']:.2f} |
| **Mediana (P50)** | {metrics['http_req_duration']['med']:.2f} |
| **P90** | {metrics['http_req_duration']['p90']:.2f} |
| **P95** | {metrics['http_req_duration']['p95']:.2f} |
| **P99** | {metrics['http_req_duration']['p99']:.2f} |
| **M√≠nimo** | {metrics['http_req_duration']['min']:.2f} |
| **M√°ximo** | {metrics['http_req_duration']['max']:.2f} |

---

## ‚úÖ Confiabilidade

- **Taxa de Sucesso:** {100 - metrics['http_req_failed']['rate']:.2f}%
- **Taxa de Erro:** {metrics['http_req_failed']['rate']:.2f}%

---

## üåê Transfer√™ncia de Dados

- **Dados Recebidos:** {format_bytes(metrics['data_received']['count'])} ({format_bytes(metrics['data_received']['rate'])}/s)
- **Dados Enviados:** {format_bytes(metrics['data_sent']['count'])} ({format_bytes(metrics['data_sent']['rate'])}/s)

---

## üìä M√©tricas por Opera√ß√£o
"""

    # Adiciona m√©tricas customizadas se existirem
    custom_ops = {
        'user_creation_duration': 'Cria√ß√£o de Usu√°rio',
        'post_creation_duration': 'Cria√ß√£o de Post',
        'comment_creation_duration': 'Cria√ß√£o de Coment√°rio',
        'like_creation_duration': 'Cria√ß√£o de Like',
        'friendship_creation_duration': 'Cria√ß√£o de Amizade',
        'feed_load_duration': 'Carregamento de Feed',
        'get_operation_duration': 'Opera√ß√µes GET',
    }
    
    has_custom = False
    for key, label in custom_ops.items():
        if key in metrics:
            if not has_custom:
                report += "\n### Dura√ß√£o por Tipo de Opera√ß√£o\n\n"
                report += "| Opera√ß√£o | M√©dia (ms) | P95 (ms) | P99 (ms) |\n"
                report += "|----------|------------|----------|----------|\n"
                has_custom = True
            
            m = metrics[key]
            report += f"| {label} | {m['avg']:.2f} | {m['p95']:.2f} | {m['p99']:.2f} |\n"
    
    report += f"""
---

## üìâ Componentes de Lat√™ncia

| Componente | Tempo M√©dio (ms) |
|------------|------------------|
| **Bloqueio (Blocked)** | {metrics.get('http_req_blocked', {}).get('avg', 0):.2f} |
| **Conex√£o (Connecting)** | {metrics.get('http_req_connecting', {}).get('avg', 0):.2f} |
| **Espera (Waiting)** | {metrics['http_req_waiting']['avg']:.2f} |

---

## üìù Observa√ß√µes

### Pontos Fortes
- Taxa de sucesso de {100 - metrics['http_req_failed']['rate']:.2f}% indica alta confiabilidade
- Throughput de {metrics['http_reqs']['rate']:.2f} req/s demonstra boa capacidade

### √Åreas de Aten√ß√£o
- P99 em {metrics['http_req_duration']['p99']:.2f}ms pode indicar outliers ocasionais
- Lat√™ncia m√°xima de {metrics['http_req_duration']['max']:.2f}ms requer investiga√ß√£o

---

**Gerado automaticamente pelo script de an√°lise do TCC**
"""
    
    output_file = output_dir / f"relatorio_{arch_name.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    return output_file

def generate_markdown_report_comparison(micro_metrics: Dict[str, Any], mono_metrics: Dict[str, Any], 
                                       output_dir: Path) -> Path:
    """Gera relat√≥rio comparativo em Markdown"""
    
    report = f"""# üìä Relat√≥rio Comparativo - Microsservi√ßos vs Monol√≠tico

**Data de Gera√ß√£o:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}

---

## üéØ Resumo Executivo

Este relat√≥rio compara o desempenho entre as arquiteturas **Microsservi√ßos** e **Monol√≠tica** em um sistema de rede social simulada, utilizando testes de carga com K6.

### üèÜ Resultado Geral

"""

    # Calcula score geral (menor lat√™ncia + maior throughput = melhor)
    micro_score = (1000 / micro_metrics['http_req_duration']['avg']) * micro_metrics['http_reqs']['rate']
    mono_score = (1000 / mono_metrics['http_req_duration']['avg']) * mono_metrics['http_reqs']['rate']
    
    winner = "Microsservi√ßos" if micro_score > mono_score else "Monol√≠tico"
    
    report += f"""
**Arquitetura Vencedora em Performance Geral:** ‚ú® **{winner}** ‚ú®

---

## üìà Compara√ß√£o de M√©tricas Principais

### Throughput (Requisi√ß√µes por Segundo)

| M√©trica | Microsservi√ßos | Monol√≠tico | Diferen√ßa | Vencedor |
|---------|----------------|------------|-----------|----------|
"""
    
    report += generate_comparison_table(
        "Taxa de Requisi√ß√µes",
        micro_metrics['http_reqs']['rate'],
        mono_metrics['http_reqs']['rate'],
        " req/s",
        lower_is_better=False
    )
    
    report += generate_comparison_table(
        "Total de Requisi√ß√µes",
        micro_metrics['http_reqs']['count'],
        mono_metrics['http_reqs']['count'],
        "",
        lower_is_better=False
    )
    
    report += f"""
---

### ‚è±Ô∏è Lat√™ncia (Tempo de Resposta)

| M√©trica | Microsservi√ßos | Monol√≠tico | Diferen√ßa | Vencedor |
|---------|----------------|------------|-----------|----------|
"""
    
    for metric, label in [('avg', 'M√©dia'), ('med', 'Mediana (P50)'), 
                          ('p90', 'P90'), ('p95', 'P95'), ('p99', 'P99'), ('max', 'M√°xima')]:
        report += generate_comparison_table(
            label,
            micro_metrics['http_req_duration'][metric],
            mono_metrics['http_req_duration'][metric],
            " ms",
            lower_is_better=True
        )
    
    report += f"""
---

### ‚úÖ Confiabilidade

| M√©trica | Microsservi√ßos | Monol√≠tico | Diferen√ßa | Vencedor |
|---------|----------------|------------|-----------|----------|
"""
    
    micro_success = 100 - micro_metrics['http_req_failed']['rate']
    mono_success = 100 - mono_metrics['http_req_failed']['rate']
    
    report += generate_comparison_table(
        "Taxa de Sucesso",
        micro_success,
        mono_success,
        "%",
        lower_is_better=False
    )
    
    report += generate_comparison_table(
        "Taxa de Erro",
        micro_metrics['http_req_failed']['rate'],
        mono_metrics['http_req_failed']['rate'],
        "%",
        lower_is_better=True
    )
    
    report += f"""
---

### üîÑ Escalabilidade

| M√©trica | Microsservi√ßos | Monol√≠tico | Diferen√ßa | Vencedor |
|---------|----------------|------------|-----------|----------|
"""
    
    report += generate_comparison_table(
        "Itera√ß√µes Completas",
        micro_metrics['iterations']['count'],
        mono_metrics['iterations']['count'],
        "",
        lower_is_better=False
    )
    
    report += generate_comparison_table(
        "Taxa de Itera√ß√µes",
        micro_metrics['iterations']['rate'],
        mono_metrics['iterations']['rate'],
        " iter/s",
        lower_is_better=False
    )
    
    report += f"""
---

### üåê Transfer√™ncia de Dados

| M√©trica | Microsservi√ßos | Monol√≠tico |
|---------|----------------|------------|
| **Dados Recebidos** | {format_bytes(micro_metrics['data_received']['count'])} | {format_bytes(mono_metrics['data_received']['count'])} |
| **Taxa de Download** | {format_bytes(micro_metrics['data_received']['rate'])}/s | {format_bytes(mono_metrics['data_received']['rate'])}/s |
| **Dados Enviados** | {format_bytes(micro_metrics['data_sent']['count'])} | {format_bytes(mono_metrics['data_sent']['count'])} |
| **Taxa de Upload** | {format_bytes(micro_metrics['data_sent']['rate'])}/s | {format_bytes(mono_metrics['data_sent']['rate'])}/s |

---

## üìä An√°lise Comparativa Detalhada

### üöÄ Performance

"""
    
    latency_diff = calculate_percentage_diff(
        micro_metrics['http_req_duration']['avg'],
        mono_metrics['http_req_duration']['avg']
    )
    
    if latency_diff < 0:
        report += f"‚úÖ **Microsservi√ßos apresentou lat√™ncia {abs(latency_diff):.1f}% MENOR** que o monol√≠tico.\n\n"
    else:
        report += f"‚ö†Ô∏è **Microsservi√ßos apresentou lat√™ncia {latency_diff:.1f}% MAIOR** que o monol√≠tico.\n\n"
    
    throughput_diff = calculate_percentage_diff(
        micro_metrics['http_reqs']['rate'],
        mono_metrics['http_reqs']['rate']
    )
    
    if throughput_diff > 0:
        report += f"‚úÖ **Microsservi√ßos teve throughput {throughput_diff:.1f}% MAIOR** que o monol√≠tico.\n\n"
    else:
        report += f"‚ö†Ô∏è **Microsservi√ßos teve throughput {abs(throughput_diff):.1f}% MENOR** que o monol√≠tico.\n\n"
    
    report += """
### üéØ Trade-offs Identificados

#### Vantagens da Arquitetura de Microsservi√ßos

- ‚úÖ **Escalabilidade Independente:** Cada servi√ßo pode escalar individualmente conforme demanda
- ‚úÖ **Isolamento de Falhas:** Problemas em um servi√ßo n√£o afetam necessariamente os outros
- ‚úÖ **Deployment Independente:** Atualiza√ß√µes podem ser feitas sem downtime completo do sistema
- ‚úÖ **Tecnologias Heterog√™neas:** Liberdade para usar diferentes tecnologias por servi√ßo
- ‚úÖ **Times Aut√¥nomos:** Equipes podem trabalhar independentemente em diferentes servi√ßos

#### Desvantagens da Arquitetura de Microsservi√ßos

- ‚ùå **Complexidade Operacional:** Maior overhead de infraestrutura e orquestra√ß√£o
- ‚ùå **Lat√™ncia de Rede:** Comunica√ß√£o entre servi√ßos adiciona lat√™ncia
- ‚ùå **Transa√ß√µes Distribu√≠das:** Mais complexo garantir consist√™ncia ACID
- ‚ùå **Debugging Complexo:** Rastreamento de erros atrav√©s de m√∫ltiplos servi√ßos
- ‚ùå **Overhead de Comunica√ß√£o:** Serializa√ß√£o/deserializa√ß√£o e chamadas HTTP

#### Vantagens da Arquitetura Monol√≠tica

- ‚úÖ **Simplicidade:** Menor complexidade operacional e de desenvolvimento inicial
- ‚úÖ **Menor Lat√™ncia:** Chamadas internas s√£o mais r√°pidas que chamadas HTTP
- ‚úÖ **Transa√ß√µes ACID:** Mais f√°cil garantir consist√™ncia com banco √∫nico
- ‚úÖ **Debugging Simples:** Stack traces completos e logs centralizados
- ‚úÖ **Menos Overhead:** Sem overhead de rede entre componentes

#### Desvantagens da Arquitetura Monol√≠tica

- ‚ùå **Escalabilidade Limitada:** Todo o sistema precisa escalar junto
- ‚ùå **Acoplamento:** Maior risco de depend√™ncias e efeitos colaterais
- ‚ùå **Deployment Arriscado:** Qualquer mudan√ßa requer deploy completo
- ‚ùå **Tamanho:** Aplica√ß√£o pode crescer muito e ficar dif√≠cil de manter
- ‚ùå **Tecnologia √önica:** Dif√≠cil adotar novas tecnologias

---

## üéì Conclus√µes para o TCC

### Quando Usar Microsservi√ßos

A arquitetura de microsservi√ßos √© mais adequada para:

1. **Aplica√ß√µes grandes e complexas** com m√∫ltiplos dom√≠nios de neg√≥cio
2. **Times grandes e distribu√≠dos** que precisam de autonomia
3. **Requisitos de alta disponibilidade** e toler√¢ncia a falhas
4. **Necessidade de escalar componentes espec√≠ficos** independentemente
5. **Evolu√ß√£o tecnol√≥gica cont√≠nua** com necessidade de experimenta√ß√£o

### Quando Usar Monol√≠tico

A arquitetura monol√≠tica √© mais adequada para:

1. **MVPs e prot√≥tipos** que precisam ser desenvolvidos rapidamente
2. **Times pequenos** com poucos desenvolvedores
3. **Aplica√ß√µes com baixa complexidade** de dom√≠nio
4. **Requisitos de lat√™ncia cr√≠tica** onde cada milissegundo conta
5. **Recursos limitados** de infraestrutura e DevOps

### Recomenda√ß√£o Final

"""
    
    if micro_score > mono_score:
        report += """
Com base nos testes realizados, **a arquitetura de microsservi√ßos demonstrou melhor performance geral** 
para este caso de uso espec√≠fico (rede social). No entanto, √© importante considerar que:

- A complexidade operacional adicional pode n√£o justificar os ganhos de performance em cen√°rios menores
- Os custos de desenvolvimento e manuten√ß√£o s√£o significativamente maiores
- A decis√£o deve considerar n√£o apenas performance, mas tamb√©m o contexto organizacional e t√©cnico

Para o cen√°rio testado (rede social com m√∫ltiplos dom√≠nios), microsservi√ßos se mostrou uma escolha adequada.
"""
    else:
        report += """
Com base nos testes realizados, **a arquitetura monol√≠tica demonstrou melhor performance** 
para este caso de uso espec√≠fico. Isso indica que:

- A simplicidade arquitetural resultou em menor lat√™ncia
- O overhead de comunica√ß√£o entre microsservi√ßos impactou a performance
- Para aplica√ß√µes de m√©dio porte, o monol√≠tico pode ser mais eficiente

Recomenda-se iniciar com monol√≠tico modular e migrar para microsservi√ßos apenas quando:
- A aplica√ß√£o atingir escala significativa
- Houver necessidade comprovada de escalabilidade independente
- A organiza√ß√£o possuir maturidade em DevOps e orquestra√ß√£o
"""
    
    report += f"""
---

## üìÅ Arquivos de Dados

- **Microsservi√ßos:** Ver arquivos na pasta `test-results/microservices_*`
- **Monol√≠tico:** Ver arquivos na pasta `test-results/monolithic_*`

---

## üìö Refer√™ncias T√©cnicas

1. Newman, S. (2021). *Building Microservices*. O'Reilly Media.
2. Richardson, C. (2018). *Microservices Patterns*. Manning Publications.
3. Fowler, M. & Lewis, J. (2014). *Microservices: A Definition of This New Architectural Term*.
4. Spring Cloud Documentation. https://spring.io/projects/spring-cloud

---

**Gerado automaticamente pelo script de an√°lise do TCC**  
**Ferramenta de teste:** K6  
**Data:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}
"""
    
    output_file = output_dir / f"relatorio_comparativo_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    return output_file

def print_terminal_summary(micro_metrics: Dict[str, Any], mono_metrics: Optional[Dict[str, Any]] = None):
    """Exibe resumo formatado no terminal"""
    print("\n" + "=" * 70)
    print("üìä RESUMO DAS M√âTRICAS")
    print("=" * 70 + "\n")
    
    if mono_metrics:
        # Compara√ß√£o
        print("üöÄ THROUGHPUT")
        print(f"  Microsservi√ßos: {micro_metrics['http_reqs']['rate']:>10.2f} req/s")
        print(f"  Monol√≠tico:     {mono_metrics['http_reqs']['rate']:>10.2f} req/s")
        diff = calculate_percentage_diff(micro_metrics['http_reqs']['rate'], mono_metrics['http_reqs']['rate'])
        print(f"  Diferen√ßa:      {diff:>10.2f}%")
        
        print("\n‚è±Ô∏è  LAT√äNCIA M√âDIA")
        print(f"  Microsservi√ßos: {micro_metrics['http_req_duration']['avg']:>10.2f} ms")
        print(f"  Monol√≠tico:     {mono_metrics['http_req_duration']['avg']:>10.2f} ms")
        diff = calculate_percentage_diff(micro_metrics['http_req_duration']['avg'], mono_metrics['http_req_duration']['avg'])
        print(f"  Diferen√ßa:      {diff:>10.2f}%")
        
        print("\n‚úÖ TAXA DE SUCESSO")
        micro_success = 100 - micro_metrics['http_req_failed']['rate']
        mono_success = 100 - mono_metrics['http_req_failed']['rate']
        print(f"  Microsservi√ßos: {micro_success:>10.2f}%")
        print(f"  Monol√≠tico:     {mono_success:>10.2f}%")
        
        # Determina vencedor
        print("\nüèÜ VENCEDOR")
        if micro_metrics['http_req_duration']['avg'] < mono_metrics['http_req_duration']['avg']:
            print("  ‚ú® Microsservi√ßos apresentou menor lat√™ncia!")
        else:
            print("  ‚ú® Monol√≠tico apresentou menor lat√™ncia!")
    else:
        # Apenas microsservi√ßos
        print("üìä MICROSSERVI√áOS")
        print(f"  Total de Requisi√ß√µes: {micro_metrics['http_reqs']['count']:,}")
        print(f"  Taxa: {micro_metrics['http_reqs']['rate']:.2f} req/s")
        print(f"\n‚è±Ô∏è  LAT√äNCIA")
        print(f"  M√©dia: {micro_metrics['http_req_duration']['avg']:.2f} ms")
        print(f"  P95:   {micro_metrics['http_req_duration']['p95']:.2f} ms")
        print(f"  P99:   {micro_metrics['http_req_duration']['p99']:.2f} ms")
        print(f"\n‚úÖ CONFIABILIDADE")
        print(f"  Taxa de Erro:    {micro_metrics['http_req_failed']['rate']:.2f}%")
        print(f"  Taxa de Sucesso: {100 - micro_metrics['http_req_failed']['rate']:.2f}%")
        
        # Gr√°fico ASCII
        latency_data = {
            'M√©dia': micro_metrics['http_req_duration']['avg'],
            'P90': micro_metrics['http_req_duration']['p90'],
            'P95': micro_metrics['http_req_duration']['p95'],
            'P99': micro_metrics['http_req_duration']['p99'],
        }
        print(generate_ascii_chart(latency_data, "Lat√™ncias (ms)"))
    
    print("\n" + "=" * 70)

def main():
    parser = argparse.ArgumentParser(
        description='Gera relat√≥rio comparativo de testes de carga',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  %(prog)s test-results/
  %(prog)s test-results/ --micro microservices_summary.json
  %(prog)s test-results/ --micro micro.json --mono mono.json
        """
    )
    parser.add_argument('results_dir', help='Diret√≥rio com os resultados dos testes')
    parser.add_argument('--micro', help='Arquivo summary JSON dos microsservi√ßos', default=None)
    parser.add_argument('--mono', help='Arquivo summary JSON do monol√≠tico', default=None)
    
    args = parser.parse_args()
    
    results_dir = Path(args.results_dir)
    
    if not results_dir.exists():
        print(f"‚ùå Diret√≥rio n√£o encontrado: {results_dir}")
        sys.exit(1)
    
    # Procura arquivos automaticamente se n√£o especificado
    if not args.micro:
        micro_files = list(results_dir.glob('microservices_*_summary.json'))
        if micro_files:
            args.micro = str(sorted(micro_files)[-1])  # Pega o mais recente
            print(f"üìä Arquivo de microsservi√ßos: {args.micro}")
    
    if not args.mono:
        mono_files = list(results_dir.glob('monolithic_*_summary.json'))
        if mono_files:
            args.mono = str(sorted(mono_files)[-1])  # Pega o mais recente
            print(f"üìä Arquivo de monol√≠tico: {args.mono}")
    
    # Valida arquivos
    if not args.micro or not Path(args.micro).exists():
        print("‚ùå Arquivo de microsservi√ßos n√£o encontrado!")
        print("Execute primeiro: ./run-load-test.sh microservices")
        sys.exit(1)
    
    # Carrega dados
    print("\nüìÑ Carregando dados...")
    micro_summary = load_k6_summary(args.micro)
    
    if not micro_summary:
        sys.exit(1)
    
    micro_metrics = extract_metrics(micro_summary)
    
    if not micro_metrics:
        print("‚ùå Erro ao extrair m√©tricas de microsservi√ßos!")
        sys.exit(1)
    
    # Se tem dados do monol√≠tico, faz compara√ß√£o
    if args.mono and Path(args.mono).exists():
        mono_summary = load_k6_summary(args.mono)
        
        if mono_summary:
            mono_metrics = extract_metrics(mono_summary)
            
            if mono_metrics:
                # Gera relat√≥rio comparativo
                print("\nüìù Gerando relat√≥rio comparativo...")
                report_file = generate_markdown_report_comparison(micro_metrics, mono_metrics, results_dir)
                print(f"\n‚úÖ Relat√≥rio gerado: {report_file}")
                
                # Exibe resumo no terminal
                print_terminal_summary(micro_metrics, mono_metrics)
                
                print(f"\nüìÑ Relat√≥rio completo: {report_file}")
                print("\nüéì Boa sorte com seu TCC!\n")
                sys.exit(0)
    
    # Se chegou aqui, s√≥ tem dados de microsservi√ßos
    print("\n‚ö†Ô∏è  Arquivo de monol√≠tico n√£o encontrado!")
    print("Voc√™ pode executar: ./run-load-test.sh monolithic")
    print("Por enquanto, gerando relat√≥rio apenas de microsservi√ßos...\n")
    
    # Gera relat√≥rio apenas de microsservi√ßos
    report_file = generate_markdown_report_single(micro_metrics, "Microsservi√ßos", results_dir)
    print(f"\n‚úÖ Relat√≥rio gerado: {report_file}")
    
    # Exibe resumo no terminal
    print_terminal_summary(micro_metrics)
    
    print(f"\nüìÑ Relat√≥rio: {report_file}")
    print("\nüéì Execute os testes do monol√≠tico para gerar o relat√≥rio comparativo!\n")

if __name__ == '__main__':
    main()
